{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0049b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Schema de Dados para o Pipeline PEDE ---\n",
    "\n",
    "# Lista de colunas que contêm valores numéricos formatados com vírgula (padrão BR)\n",
    "COLUNAS_DECIMAIS = [\n",
    "    'INDE 2024', 'INDE 23', 'INDE 22', 'INDE',\n",
    "    'Cg', 'Cf', 'Ct', \n",
    "    'IAA', 'IEG', 'IPS', 'IPP', 'IDA', 'IPV', 'IAN', \n",
    "    'Mat', 'Por', 'Ing', \n",
    "    'Rec Av1', 'Rec Av2', 'Rec Psicologia'\n",
    "]\n",
    "\n",
    "# Dicionário de tipos para carregamento otimizado (evita inferência errada)\n",
    "SCHEMA_DTYPES = {\n",
    "    'RA': str,                 # ID como string para evitar perda de zeros à esquerda se houver\n",
    "    'Fase': str,\n",
    "    'Fase Ideal': str,\n",
    "    'Pedra': str,\n",
    "    'Pedra 2024': str,\n",
    "    'Pedra 23': str,\n",
    "    'Pedra 22': str,\n",
    "    'Pedra 21': str,\n",
    "    'Pedra 20': str,\n",
    "    'Turma': str,\n",
    "    'Nome Anonimizado': str,\n",
    "    'Gênero': str,\n",
    "    'Instituição de ensino': str,\n",
    "    'Escola': str,\n",
    "    'Defasagem': float,        # Usamos float pois pode conter NaNs\n",
    "    'Nº Av': float,\n",
    "    'Ano ingresso': float,\n",
    "    'Idade': float,\n",
    "    'Ativo/ Inativo': str,\n",
    "    'Avaliador1': str, 'Avaliador2': str, 'Avaliador3': str, \n",
    "    'Avaliador4': str, 'Avaliador5': str, 'Avaliador6': str,\n",
    "    'Destaque IEG': str, 'Destaque IDA': str, 'Destaque IPV': str\n",
    "}\n",
    "\n",
    "# Colunas que devem ser parseadas como data\n",
    "COLUNAS_DATA = ['Data de Nasc']\n",
    "\n",
    "def converter_decimal_ptbr(valor):\n",
    "    \"\"\"\n",
    "    Converte strings numéricas no formato PT-BR (1.000,00) para float Python.\n",
    "    Trata valores nulos e erros como NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(valor) or valor == '' or str(valor).strip() == '#N/A':\n",
    "        return np.nan\n",
    "        \n",
    "    if isinstance(valor, (int, float)):\n",
    "        return float(valor)\n",
    "        \n",
    "    try:\n",
    "        # Remove pontos de milhar e troca vírgula decimal por ponto\n",
    "        limpo = str(valor).replace('.', '').replace(',', '.')\n",
    "        return float(limpo)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Criando dicionário de converters para usar no pd.read_csv\n",
    "CONVERTERS_DECIMAIS = {col: converter_decimal_ptbr for col in COLUNAS_DECIMAIS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b95732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema de mapeamento dos nomes de colunas (Variações -> Nome Padrão do Dicionário)\n",
    "# Estrutura: \"Nome Padrão\": [\"Variação 1\", \"Variação 2\", ...]\n",
    "\n",
    "DICTIONARY_MAPPING = {\n",
    "    \"columns\": {\n",
    "        \"Ano Ingresso\": [\"Ano ingresso\"],\n",
    "        \"Data Nascimento\": [\"Ano nasc\", \"Data de Nasc\"],\n",
    "        \"Atingiu PV\": [\"Atingiu PV\"],\n",
    "        \"Avaliador 1\": [\"Avaliador1\"],\n",
    "        \"Avaliador 2\": [\"Avaliador2\"],\n",
    "        \"Avaliador 3\": [\"Avaliador3\"],\n",
    "        \"Avaliador 4\": [\"Avaliador4\"],\n",
    "        \"Avaliador 5\": [\"Avaliador5\"],\n",
    "        \"Avaliador 6\": [\"Avaliador6\"],\n",
    "        \"Cf\": [\"Cf\"],\n",
    "        \"Cg\": [\"Cg\"],\n",
    "        \"Ct\": [\"Ct\"],\n",
    "        \"Defasagem\": [\"Defas\", \"Defasagem\"],\n",
    "        \"Destaque IDA\": [\"Destaque IDA\"],\n",
    "        \"Destaque IEG\": [\"Destaque IEG\"],\n",
    "        \"Destaque IPV\": [\"Destaque IPV\"],\n",
    "        \"Escola\": [\"Escola\"],\n",
    "        \"Fase\": [\"Fase\"],\n",
    "        \"Fase Ideal\": [\"Fase ideal\", \"Fase Ideal\"],\n",
    "        \"Gênero\": [\"Gênero\"],\n",
    "        \"IAA\": [\"IAA\"],\n",
    "        \"IAN\": [\"IAN\"],\n",
    "        \"IDA\": [\"IDA\"],\n",
    "        \"Idade\": [\"Idade 22\", \"Idade\"],\n",
    "        \"IEG\": [\"IEG\"],\n",
    "        # Mapeando qualquer INDE principal (ano corrente da planilha) para \"INDE\"\n",
    "        \"INDE\": [\"INDE 22\", \"INDE 23\", \"INDE 2023\", \"INDE 2024\", \"INDE\"],\n",
    "        \"Indicado\": [\"Indicado\"],\n",
    "        \"Inglês\": [\"Inglês\", \"Ing\"],\n",
    "        \"Instituição de Ensino\": [\"Instituição de ensino\"],\n",
    "        \"IPP\": [\"IPP\"],\n",
    "        \"IPS\": [\"IPS\"],\n",
    "        \"IPV\": [\"IPV\"],\n",
    "        \"Matemática\": [\"Matem\", \"Mat\"],\n",
    "        \"Nome\": [\"Nome\", \"Nome Anonimizado\"],\n",
    "        \"Nº Av\": [\"Nº Av\"],\n",
    "        \"Pedra 20\": [\"Pedra 20\"],\n",
    "        \"Pedra 21\": [\"Pedra 21\"],\n",
    "        \"Pedra 22\": [\"Pedra 22\"],\n",
    "        \"Pedra 23\": [\"Pedra 23\", \"Pedra 2023\"],\n",
    "        \"Pedra 24\": [\"Pedra 2024\"],\n",
    "        \"Português\": [\"Portug\", \"Por\"],\n",
    "        \"RA\": [\"RA\"],\n",
    "        \"Rec Av1\": [\"Rec Av1\"],\n",
    "        \"Rec Av2\": [\"Rec Av2\"],\n",
    "        \"Rec Av3\": [\"Rec Av3\"],\n",
    "        \"Rec Av4\": [\"Rec Av4\"],\n",
    "        \"Rec Psicologia\": [\"Rec Psicologia\"],\n",
    "        \"Turma\": [\"Turma\"],\n",
    "        \"Ativo/Inativo\": [\"Ativo/ Inativo\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Função auxiliar para renomear colunas do DF baseado no mapping\n",
    "def padronizar_colunas(df, mapping):\n",
    "    col_map = {}\n",
    "    for padrao, variacoes in mapping['columns'].items():\n",
    "        for var in variacoes:\n",
    "            if var in df.columns:\n",
    "                col_map[var] = padrao\n",
    "    \n",
    "    return df.rename(columns=col_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77a183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tratar colunas duplicadas no carregamento\n",
    "def remover_colunas_duplicadas(df):\n",
    "    \"\"\"\n",
    "    Remove colunas duplicadas de um DataFrame.\n",
    "    Se houver colunas com o mesmo nome (ex: 'Ativo/ Inativo', 'Ativo/ Inativo.1'),\n",
    "    mantém apenas a primeira ocorrência.\n",
    "    \"\"\"\n",
    "    # Identifica colunas duplicadas (pelo nome original ou pandas sufixos)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    print(f\"Colunas após remoção de duplicatas exatas: {df.shape[1]}\")\n",
    "    \n",
    "    # Tratamento específico para sufixos do Pandas (.1, .2) se o usuário carregar sem mangle_dupe_cols=True\n",
    "    # Mas o padrão do pandas é renomear. Vamos limpar esses nomes se forem cópias.\n",
    "    cols_originais = [c.split('.')[0] for c in df.columns]\n",
    "    \n",
    "    # Se quiser forçar a remoção de colunas que o Pandas renomeou (ex: Coluna.1)\n",
    "    # Verificamos se o \"tronco\" do nome já existe antes\n",
    "    cols_unicas = []\n",
    "    seen = set()\n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for i, col in enumerate(df.columns):\n",
    "        nome_base = col.split('.')[0] # Remove sufixo .1, .2 gerado pelo pandas\n",
    "        \n",
    "        # Lógica: Se o nome base já foi visto E o pandas adicionou sufixo numérico\n",
    "        if nome_base in seen and col != nome_base:\n",
    "            # Verifica se o conteúdo é igual ao original\n",
    "            if df[nome_base].equals(df[col]):\n",
    "                cols_to_drop.append(col)\n",
    "                # print(f\"Coluna duplicada removida: {col} (cópia idêntica de {nome_base})\")\n",
    "            else:\n",
    "                # Se conteúdo diferente, mantém mas avisa\n",
    "                # print(f\"Aviso: Coluna {col} tem nome similar a {nome_base} mas conteúdo diferente. Mantida.\")\n",
    "                pass\n",
    "        else:\n",
    "            seen.add(nome_base)\n",
    "            \n",
    "    if cols_to_drop:\n",
    "        print(f\"Removendo {len(cols_to_drop)} colunas duplicadas pelo Pandas: {cols_to_drop}\")\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ed21f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\angélica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\angélica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\angélica\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\angélica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\angélica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\angélica\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\angélica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\angélica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru in c:\\users\\angélica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\angélica\\appdata\\roaming\\python\\python312\\site-packages (from loguru) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\angélica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from loguru) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Libs\n",
    "%pip install pandas\n",
    "%pip install openpyxl\n",
    "%pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943b9b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 16:39:08.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mThis is an info message\u001b[0m\n",
      "\u001b[32m2026-02-16 16:39:08.509\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[33m\u001b[1mThis is a warning\u001b[0m\n",
      "\u001b[32m2026-02-16 16:39:08.511\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[31m\u001b[1mThis is an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "\n",
    " \n",
    "logger.add(\n",
    "    \"app.log\",   \n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    "    level=\"DEBUG\"  # Nível de log\n",
    ")\n",
    " \n",
    "\n",
    "logger.info(\"This is an info message\")\n",
    "logger.warning(\"This is a warning\")\n",
    "logger.error(\"This is an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7aa5381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Carregando 2022 ---\n",
      "Colunas após remoção de duplicatas exatas: 42\n",
      "Shape: (860, 42)\n",
      "\n",
      "--- Carregando 2023 ---\n",
      "Colunas após remoção de duplicatas exatas: 48\n",
      "Removendo 1 colunas duplicadas pelo Pandas: ['Destaque IPV.1']\n",
      "Shape: (1014, 47)\n",
      "\n",
      "--- Carregando 2024 ---\n",
      "Colunas após remoção de duplicatas exatas: 50\n",
      "Removendo 1 colunas duplicadas pelo Pandas: ['Ativo/ Inativo.1']\n",
      "Shape: (1156, 49)\n",
      "\n",
      "--- Padronizando Colunas ---\n",
      "\n",
      "Colunas comuns nos 3 anos: 40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "# Caminho do arquivo\n",
    "file = \"C:/Users/Angélica/Desktop/datathon/projeto_datathon/arquivos/BASE DE DADOS PEDE 2024 - DATATHON.xlsx\"\n",
    "\n",
    "# Função auxiliar de carregamento e limpeza inicial\n",
    "def carregar_aba(excel_file, sheet_name):\n",
    "    # Carrega dados\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    \n",
    "    # Remove espaços em branco dos nomes das colunas\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Remove colunas duplicadas imediatamente\n",
    "    df = remover_colunas_duplicadas(df)\n",
    "    \n",
    "    # Tratamentos padrão de RA\n",
    "    if 'RA' in df.columns:\n",
    "        df['RA'] = df['RA'].astype(str).str.zfill(7)\n",
    "        \n",
    "    return df\n",
    "\n",
    "print(\"--- Carregando 2022 ---\")\n",
    "df_2022 = carregar_aba(file, \"PEDE2022\")\n",
    "print(f\"Shape: {df_2022.shape}\")\n",
    "\n",
    "print(\"\\n--- Carregando 2023 ---\")\n",
    "df_2023 = carregar_aba(file, \"PEDE2023\")\n",
    "print(f\"Shape: {df_2023.shape}\")\n",
    "\n",
    "print(\"\\n--- Carregando 2024 ---\")\n",
    "df_2024 = carregar_aba(file, \"PEDE2024\")\n",
    "print(f\"Shape: {df_2024.shape}\")\n",
    "\n",
    "# Padronização de nomes (usando o mapping definido anteriormente)\n",
    "# IMPORTANTE: Definir DICTIONARY_MAPPING antes de rodar isso (célula anterior)\n",
    "if 'padronizar_colunas' in locals():\n",
    "    print(\"\\n--- Padronizando Colunas ---\")\n",
    "    df_2022 = padronizar_colunas(df_2022, DICTIONARY_MAPPING)\n",
    "    df_2023 = padronizar_colunas(df_2023, DICTIONARY_MAPPING)\n",
    "    df_2024 = padronizar_colunas(df_2024, DICTIONARY_MAPPING)\n",
    "\n",
    "# Verificar intersecção após padronização\n",
    "cols_comuns = set(df_2022.columns) & set(df_2023.columns) & set(df_2024.columns)\n",
    "print(f\"\\nColunas comuns nos 3 anos: {len(cols_comuns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8677163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando padronização de nomes...\n",
      "=== RELATÓRIO DE COMPARAÇÃO DE ESTRUTURA ===\n",
      "Total de colunas únicas encontradas (união): 49\n",
      "Colunas comuns a todos os anos: 40\n",
      "----------------------------------------\n",
      "⚠️ Diferenças entre 2022 e 2023:\n",
      "   - Exclusivas em 2023: 2 colunas (Pedra 23, IPP...)\n",
      "⚠️ Diferenças entre 2023 e 2024:\n",
      "   - Exclusivas em 2023: 2 colunas (Rec Av4, Rec Av3...)\n",
      "   - Exclusivas em 2024: 5 colunas (Pedra 24, Ativo/Inativo, Escola, Avaliador 6, Avaliador 5...)\n",
      "----------------------------------------\n",
      "\n",
      "=== VALIDAÇÃO DE CONTEÚDO (Colunas Comuns) ===\n",
      "⚠️ Divergência de Tipo na coluna 'Atingiu PV': {'2022': 'object', '2023': 'float64', '2024': 'float64'}\n",
      "⚠️ Divergência de Tipo na coluna 'Cf': {'2022': 'int64', '2023': 'float64', '2024': 'float64'}\n",
      "⚠️ Divergência de Tipo na coluna 'Cg': {'2022': 'int64', '2023': 'float64', '2024': 'float64'}\n",
      "⚠️ Divergência de Tipo na coluna 'Ct': {'2022': 'int64', '2023': 'float64', '2024': 'float64'}\n",
      "⚠️ Divergência de Tipo na coluna 'Data Nascimento': {'2022': 'int64', '2023': 'object', '2024': 'datetime64[ns]'}\n",
      "⚠️ Divergência de Tipo na coluna 'Destaque IDA': {'2022': 'object', '2023': 'float64', '2024': 'float64'}\n",
      "⚠️ Divergência de Tipo na coluna 'Destaque IEG': {'2022': 'object', '2023': 'float64', '2024': 'float64'}\n",
      "⚠️ Divergência de Tipo na coluna 'Destaque IPV': {'2022': 'object', '2023': 'float64', '2024': 'float64'}\n",
      "⚠️ Divergência de Tipo na coluna 'Fase': {'2022': 'int64', '2023': 'object', '2024': 'object'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_3228\\728786052.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     82\u001b[39m         \u001b[33m'2023'\u001b[39m: df_2023_std,\n\u001b[32m     83\u001b[39m         \u001b[33m'2024'\u001b[39m: df_2024_std\n\u001b[32m     84\u001b[39m     }\n\u001b[32m     85\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     relatorio_comparativo(dict_dfs)\n\u001b[32m     87\u001b[39m \n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Atualizando as variáveis globais para as versões padronizadas (opcional)\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# df = pd.concat(dict_dfs.values(), ignore_index=True) # Se quiser concatenar já\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_3228\\728786052.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(dfs_dict)\u001b[39m\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;28;01min\u001b[39;00m sorted(list(colunas_comuns)):\n\u001b[32m     44\u001b[39m         tipos = {}\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m ano, df \u001b[38;5;28;01min\u001b[39;00m dfs_dict.items():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m             tipos[ano] = str(df[col].dtype)\n\u001b[32m     47\u001b[39m \n\u001b[32m     48\u001b[39m         \u001b[38;5;66;03m# Se houver mais de um tipo diferente para a mesma coluna\u001b[39;00m\n\u001b[32m     49\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(set(tipos.values())) > \u001b[32m1\u001b[39m:\n",
      "\u001b[32mc:\\Users\\Angélica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6314\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6315\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6316\u001b[39m         ):\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6318\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "# --- Padronização e Validação Comparativa entre Anos ---\n",
    "\n",
    "def relatorio_comparativo(dfs_dict):\n",
    "    \"\"\"\n",
    "    Compara colunas e conteúdo entre múltiplos DataFrames (anos).\n",
    "    dfs_dict: dicionário {'2022': df22, '2023': df23, ...}\n",
    "    \"\"\"\n",
    "    print(\"=== RELATÓRIO DE COMPARAÇÃO DE ESTRUTURA ===\")\n",
    "    \n",
    "    anos = list(dfs_dict.keys())\n",
    "    cols_sets = {ano: set(df.columns) for ano, df in dfs_dict.items()}\n",
    "    \n",
    "    # 1. Padronização (já aplicada antes desta chamada, mas verificando colunas resultantes)\n",
    "    todas_colunas = set().union(*cols_sets.values())\n",
    "    colunas_comuns = set.intersection(*cols_sets.values())\n",
    "    \n",
    "    print(f\"Total de colunas únicas encontradas (união): {len(todas_colunas)}\")\n",
    "    print(f\"Colunas comuns a todos os anos: {len(colunas_comuns)}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # 2. Diferenças entre anos adjacentes\n",
    "    for i in range(len(anos) - 1):\n",
    "        ano_a = anos[i]\n",
    "        ano_b = anos[i+1]\n",
    "        \n",
    "        diff_a = cols_sets[ano_a] - cols_sets[ano_b]\n",
    "        diff_b = cols_sets[ano_b] - cols_sets[ano_a]\n",
    "        \n",
    "        if not diff_a and not diff_b:\n",
    "            print(f\"✅ {ano_a} e {ano_b} possuem exatamente as mesmas colunas.\")\n",
    "        else:\n",
    "            print(f\"⚠️ Diferenças entre {ano_a} e {ano_b}:\")\n",
    "            if diff_a:\n",
    "                print(f\"   - Exclusivas em {ano_a}: {len(diff_a)} colunas ({', '.join(list(diff_a)[:5])}...)\")\n",
    "            if diff_b:\n",
    "                print(f\"   - Exclusivas em {ano_b}: {len(diff_b)} colunas ({', '.join(list(diff_b)[:5])}...)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 3. Validação de Conteúdo (Tipos) para colunas comuns\n",
    "    print(\"\\n=== VALIDAÇÃO DE CONTEÚDO (Colunas Comuns) ===\")\n",
    "    divergencias_tipo = []\n",
    "    \n",
    "    for col in sorted(list(colunas_comuns)):\n",
    "        tipos = {}\n",
    "        for ano, df in dfs_dict.items():\n",
    "            tipos[ano] = str(df[col].dtype)\n",
    "        \n",
    "        # Se houver mais de um tipo diferente para a mesma coluna\n",
    "        if len(set(tipos.values())) > 1:\n",
    "            divergencias_tipo.append((col, tipos))\n",
    "            print(f\"⚠️ Divergência de Tipo na coluna '{col}': {tipos}\")\n",
    "    \n",
    "    if not divergencias_tipo:\n",
    "        print(\"✅ Tipos de dados consistentes nas colunas comuns.\")\n",
    "        \n",
    "    print(\"\\n=== VALIDAÇÃO DE NULOS (Colunas Comuns) ===\")\n",
    "    # Exibir % de nulos para verificar se uma colunas \"sumiu\" conceitualmente (ficou vazia)\n",
    "    for col in sorted(list(colunas_comuns)):\n",
    "        msgs = []\n",
    "        alerta = False\n",
    "        for ano in anos:\n",
    "            pct_null = dfs_dict[ano][col].isnull().mean() * 100\n",
    "            if pct_null > 90: # Se mais de 90% for nulo, alerta\n",
    "                alerta = True\n",
    "            msgs.append(f\"{ano}: {pct_null:.1f}%\")\n",
    "        \n",
    "        if alerta:\n",
    "            print(f\"⚠️ Coluna '{col}' quase vazia em algum ano: \" + \" | \".join(msgs))\n",
    "            \n",
    "    print(\"\\nConclusão da Validação Comparativa Finalizada.\")\n",
    "\n",
    "# Aplicando a padronização antes de comparar\n",
    "if 'df_2022' in locals() and 'df_2023' in locals() and 'df_2024' in locals():\n",
    "    print(\"Aplicando padronização de nomes...\")\n",
    "    df_2022_std = padronizar_colunas(df_2022, DICTIONARY_MAPPING)\n",
    "    df_2023_std = padronizar_colunas(df_2023, DICTIONARY_MAPPING)\n",
    "    df_2024_std = padronizar_colunas(df_2024, DICTIONARY_MAPPING)\n",
    "    \n",
    "    # Dicionário para a função de relatório\n",
    "    dict_dfs = {\n",
    "        '2022': df_2022_std,\n",
    "        '2023': df_2023_std,\n",
    "        '2024': df_2024_std\n",
    "    }\n",
    "    \n",
    "    relatorio_comparativo(dict_dfs)\n",
    "    \n",
    "    # Atualizando as variáveis globais para as versões padronizadas (opcional)\n",
    "    # df = pd.concat(dict_dfs.values(), ignore_index=True) # Se quiser concatenar já\n",
    "else:\n",
    "    print(\"DataFrames não carregados. Execute a célula de leitura primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9934b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 16:41:13.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mAplicação LeituraArquivos inicializada\u001b[0m\n",
      "\u001b[32m2026-02-16 16:41:13.026\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.utils\u001b[0m:\u001b[36mlog_method_call\u001b[0m:\u001b[36m153\u001b[0m - \u001b[34m\u001b[1mler_arquivo(caminho=C:\\Users\\Angélica\\Desktop\\datathon\\projeto_datathon\\arquivos\\BASE DE DADOS PEDE 2024 - DATATHON.xlsx)\u001b[0m\n",
      "\u001b[32m2026-02-16 16:41:13.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mler_arquivo\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mIniciando leitura de itens\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 16:41:13.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mler_arquivo\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mDados carregados com sucesso. Shape: (860, 42). Colunas: ['RA', 'Fase', 'Turma', 'Nome', 'Ano nasc', 'Idade 22', 'Gênero', 'Ano ingresso', 'Instituição de ensino', 'Pedra 20', 'Pedra 21', 'Pedra 22', 'INDE 22', 'Cg', 'Cf', 'Ct', 'Nº Av', 'Avaliador1', 'Rec Av1', 'Avaliador2', 'Rec Av2', 'Avaliador3', 'Rec Av3', 'Avaliador4', 'Rec Av4', 'IAA', 'IEG', 'IPS', 'Rec Psicologia', 'IDA', 'Matem', 'Portug', 'Inglês', 'Indicado', 'Atingiu PV', 'IPV', 'IAN', 'Fase ideal', 'Defas', 'Destaque IEG', 'Destaque IDA', 'Destaque IPV']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Adiciona o diretório raiz do projeto ao sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.utils import LoggerConfig, ApplicationLogger\n",
    "\n",
    "class LeituraArquivos:\n",
    "    def __init__(self, name: str = \"LeituraArquivos\", caminho: str = \"data/\"):\n",
    "        \"\"\"\n",
    "        Inicializa a aplicação\n",
    "        \n",
    "        Args:\n",
    "            name: Nome da aplicação\n",
    "            caminho: Caminho padrão para os arquivos\n",
    "        \"\"\"\n",
    "        # Configuração do logger\n",
    "        self.config = LoggerConfig(\n",
    "            app_name=name,\n",
    "            log_dir=f\"logs/{name.lower()}\"\n",
    "        )\n",
    "        \n",
    "        self.logger = ApplicationLogger(self.__class__.__name__, self.config)\n",
    "        \n",
    "        self.logger.logger.info(f\"Aplicação {name} inicializada\")\n",
    "        self.caminho = caminho\n",
    "        \n",
    "    def ler_arquivo(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Lê um arquivo CSV ou Excel e retorna um DataFrame\n",
    "        \n",
    "        Args:\n",
    "            caminho: Caminho do arquivo a ser lido\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame com os dados lidos\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.log_method_call(\"ler_arquivo\", caminho=self.caminho)\n",
    "\n",
    "        if not os.path.exists(self.caminho):\n",
    "            self.logger.log_exception(\"ler_arquivo\", e)\n",
    "            self.logger.logger.error(f\"Arquivo não encontrado: {self.caminho}\") \n",
    "            raise FileNotFoundError(f\"Arquivo não encontrado: {self.caminho}\")\n",
    "        \n",
    "        self.logger.logger.info(f\"Iniciando leitura de itens\")\n",
    "\n",
    "        if self.caminho.endswith('.xlsx'):\n",
    "            df = pd.read_excel(self.caminho)\n",
    "        else:\n",
    "            self.logger.logger.error(f\"Formato de arquivo não suportado: {self.caminho}\")\n",
    "            self.logger.log_exception(\"ler_arquivo\", e)\n",
    "            raise ValueError(\"Formato de arquivo não suportado. Use .xlsx ou .csv\")\n",
    "\n",
    "        self.logger.logger.info(f\"Dados carregados com sucesso. Shape: {df.shape}. Colunas: {df.columns.tolist()}\")\n",
    "        return df\n",
    "\n",
    "leitura = LeituraArquivos(caminho=\"C:\\\\Users\\\\Angélica\\\\Desktop\\\\datathon\\\\projeto_datathon\\\\arquivos\\\\BASE DE DADOS PEDE 2024 - DATATHON.xlsx\")\n",
    "df = leitura.ler_arquivo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1c9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cc3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5e709cf",
   "metadata": {},
   "source": [
    "# Validação de Dados e Modelagem Preditiva\n",
    "\n",
    "Nesta seção, realizaremos:\n",
    "1. **Validação dos Dados**: Verificação de consistência, valores nulos e distribuições.\n",
    "2. **Engenharia de Features**: Preparação das variáveis para o modelo.\n",
    "3. **Modelo Preditivo**: Criação de um modelo para estimar o risco de defasagem escolar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d567b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Configuração de visualização\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "def validar_dados(df, ano):\n",
    "    \"\"\"\n",
    "    Realiza validações básicas no DataFrame e retorna um relatório.\n",
    "    \"\"\"\n",
    "    print(f\"--- Relatório de Validação: {ano} ---\")\n",
    "    \n",
    "    # 1. Verificar Nulos em Colunas Críticas\n",
    "    cols_criticas = ['RA', 'Fase', 'INDE 2024', 'Pedra 2024', 'Defasagem', 'IAA', 'IEG', 'IPS', 'IPP', 'IDA', 'IPV', 'IAN']\n",
    "    # Ajuste para colunas que podem variar de nome dependendo do ano (ex: INDE 2024 vs INDE 2023)\n",
    "    # A padronização já deve ter cuidado disso, mas vamos garantir\n",
    "    cols_existentes = [c for c in cols_criticas if c in df.columns]\n",
    "    \n",
    "    if not cols_existentes:\n",
    "        print(\"⚠️ Nenhuma coluna crítica encontrada. Verifique se a padronização foi aplicada.\")\n",
    "        return\n",
    "\n",
    "    nulos = df[cols_existentes].isnull().sum()\n",
    "    if nulos.sum() > 0:\n",
    "        print(\"⚠️ Atenção: Valores nulos encontrados:\")\n",
    "        print(nulos[nulos > 0])\n",
    "    else:\n",
    "        print(\"✅ Colunas críticas sem valores nulos.\")\n",
    "\n",
    "    # 2. Verificar Duplicidade de RA\n",
    "    if 'RA' in df.columns:\n",
    "        duplicados = df['RA'].duplicated().sum()\n",
    "        if duplicados > 0:\n",
    "            print(f\"⚠️ Atenção: {duplicados} RAs duplicados encontrados.\")\n",
    "        else:\n",
    "            print(\"✅ ID (RA) únicos validados.\")\n",
    "            \n",
    "    # 3. Validar Consistência da Coluna Defasagem\n",
    "    if 'Defasagem' in df.columns:\n",
    "        # Converter para numérico se necessário, forçando NaN em erros\n",
    "        # Usamos uma cópia para não alterar o DF original aqui se não for intenção\n",
    "        defasagem_check = pd.to_numeric(df['Defasagem'], errors='coerce')\n",
    "        print(\"Distribuição da Defasagem:\")\n",
    "        print(defasagem_check.value_counts().sort_index())\n",
    "    \n",
    "    print(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "# Assumindo que df_2024 já foi carregado nas células anteriores\n",
    "if 'df_2024' in locals():\n",
    "    validar_dados(df_2024, \"2024\")\n",
    "else:\n",
    "    print(\"DataFrame df_2024 não encontrado na memória.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05f4d5c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y, cols_to_use\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdf_2024\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# Modelagem\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     X, y, feature_names = \u001b[43mpreparar_dados_modelo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_2024\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     63\u001b[39m         \u001b[38;5;66;03m# Split\u001b[39;00m\n\u001b[32m     64\u001b[39m         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u001b[32m0.3\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mpreparar_dados_modelo\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Imputar valores faltantes nas features com a mediana\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m imputer = \u001b[43mSimpleImputer\u001b[49m(strategy=\u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     53\u001b[39m X = imputer.fit_transform(data_model[cols_to_use])\n\u001b[32m     54\u001b[39m y = data_model[\u001b[33m'\u001b[39m\u001b[33mTarget_Risco\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'SimpleImputer' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Preparação para o Modelo Preditivo ---\n",
    "import numpy as np\n",
    "\n",
    "def preparar_dados_modelo(df):\n",
    "    \"\"\"\n",
    "    Prepara o DataFrame para o treinamento do modelo.\n",
    "    \"\"\"\n",
    "    # 1. Seleção de Features\n",
    "    features = ['IAA', 'IEG', 'IPS', 'IPP', 'IDA', 'IPV', 'IAN', 'Idade']\n",
    "    target = 'Defasagem'\n",
    "    \n",
    "    # Verificar se target existe\n",
    "    if target not in df.columns:\n",
    "        print(f\"Erro: Coluna alvo '{target}' não encontrada no DataFrame.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Validação se colunas de features existem\n",
    "    cols_to_use = [col for col in features if col in df.columns]\n",
    "    \n",
    "    if not cols_to_use:\n",
    "        print(\"Erro: Nenhuma feature encontrada no DataFrame.\")\n",
    "        return None, None, None\n",
    "\n",
    "    data_model = df[cols_to_use + [target]].copy()\n",
    "    \n",
    "    # 2. Tratamento de Dados\n",
    "    # Converter colunas numéricas (empties ou strings com vírgula)\n",
    "    for col in cols_to_use + [target]: # Incluindo target na conversão se necessário\n",
    "        if data_model[col].dtype == 'object':\n",
    "            # Corrige a substituição: replace deve ser aplicado na string, e lidamos com NaN\n",
    "            data_model[col] = data_model[col].astype(str).str.replace(',', '.', regex=False)\n",
    "            data_model[col] = data_model[col].replace(['#N/A', 'nan', 'None', ''], np.nan)\n",
    "        \n",
    "        data_model[col] = pd.to_numeric(data_model[col], errors='coerce')\n",
    "    \n",
    "    # Tratamento do Target\n",
    "    # Definindo 'Risco' como Defasagem < 0\n",
    "    # Precisamos garantir que não haja NaNs no target antes de criar a classe\n",
    "    data_model.dropna(subset=[target], inplace=True)\n",
    "    \n",
    "    data_model['Target_Risco'] = np.where(data_model['Defasagem'] < 0, 1, 0)\n",
    "    \n",
    "    # Remover linhas onde todas as features são nulas\n",
    "    data_model.dropna(subset=cols_to_use, how='all', inplace=True)\n",
    "    \n",
    "    # Se sobrou dado\n",
    "    if data_model.empty:\n",
    "        print(\"Erro: DataFrame vazio após tratamento.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Imputar valores faltantes nas features com a mediana\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X = imputer.fit_transform(data_model[cols_to_use])\n",
    "    y = data_model['Target_Risco']\n",
    "    \n",
    "    return X, y, cols_to_use\n",
    "\n",
    "if 'df_2024' in locals():\n",
    "    # Modelagem\n",
    "    X, y, feature_names = preparar_dados_modelo(df_2024)\n",
    "    \n",
    "    if X is not None and y is not None:\n",
    "        # Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Treino\n",
    "        modelo = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        modelo.fit(X_train, y_train)\n",
    "        \n",
    "        # Predição\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        \n",
    "        # Avaliação\n",
    "        print(\"--- Resultados do Modelo de Risco de Defasagem ---\")\n",
    "        print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred, target_names=['Sem Risco', 'Em Risco']))\n",
    "        \n",
    "        # Importância das Variáveis\n",
    "        importancia = pd.DataFrame({'Feature': feature_names, 'Importance': modelo.feature_importances_})\n",
    "        importancia = importancia.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importancia, palette='viridis')\n",
    "        plt.title('Importância das Variáveis na Predição do Risco')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Não foi possível treinar o modelo devido a erros na preparação dos dados.\")\n",
    "else:\n",
    "    print(\"Carregue df_2024 para executar o modelo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
